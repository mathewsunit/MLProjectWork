{
    "collab_server" : "",
    "contents" : "#Complete Code for the Implementation of classifiers\n#packages required\nrequire(randomForest)\nrequire(caret)\nrequire(ipred)\nrequire(e1071)\nrequire(plyr)\nrequire(knitr)\nrequire(Amelia)\nrequire(gbm)\nrequire(class)\nrequire(h2o)\n#loading the dataset\ntrain_values_url <- \"http://s3.amazonaws.com/drivendata/data/7/public/4910797b-ee55-40a7-8668-10efd5c1b960.csv\"\ntrain_values <- read.csv(train_values_url)\ntrain_labels_url <- \"http://s3.amazonaws.com/drivendata/data/7/public/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv\"\ntrain_labels <- read.csv(train_labels_url)\ntest_values_url <- \"http://s3.amazonaws.com/drivendata/data/7/public/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv\"\ntest_values <- read.csv(test_values_url)\ntrain <- merge(train_labels, train_values)\n#preprocessing of dataset\ntrain_modified <- sapply(train,as.numeric)\ntrain_modified <- as.data.frame(train_modified)\nnew_train <- subset(train_modified, select = c(amount_tsh,gps_height,longitude,latitude,basin,region_code,population,permit,construction_year,extraction_type,quantity_group,source_class,waterpoint_type,status_group))\n#standardizng the dataset\ntrain_normalize <- preProcess(new_train[,-14],method = c(\"center\",\"scale\"), na.remove = TRUE)\nindex_train<- apply(train, 2, function(x) any(is.na(x)))\nindex_train<- apply(train, 2, function(x) any(is.nan(x)))\nindex_train<- apply(train, 2, function(x) any(is.null(x)))\nindex_test<- apply(test_values, 2, function(x) any(is.na(x)))\nindex_test<- apply(test_values, 2, function(x) any(is.nan(x)))\nindex_test<- apply(test_values, 2, function(x) any(is.null(x)))\n#feature engineering\n# Observe the installer variable\nsummary(train$installer)\n# Make installer lowercase, take first 3 letters as a sub string\ntrain$install_3 <- substr(tolower(train$installer),1,3)\ntrain$install_3[train$install_3 %in% c(\" \", \"\", \"0\", \"_\", \"-\")] <- \"other\"\n# Take the top 15 substrings from above by occurance frequency\ninstall_top_15 <- names(summary(as.factor(train$install_3)))[1:15]\ntrain$install_3[!(train$install_3 %in% install_top_15)] <- \"other\"\ntrain$install_3 <- as.factor(train$install_3)\n# Table of the install_3 variable vs the status of the pumps\ntable(train$install_3, train$status_group)\n# As row-wise proportions, install_3 vs status_group\nprop.table(table(train$install_3, train$status_group ), margin = 1)\n# Create install_3 for the test set using same top 15 from above\ntest_values$install_3 <- substr(tolower(test_values$installer),1,3)\ntest_values$install_3[test_values$install_3 %in% c(\" \", \"\", \"0\", \"_\", \"-\")] <- \"other\"\ntest_values$install_3[!(test_values$install_3 %in% install_top_15)] <- \"other\"\ntest_values$install_3 <- as.factor(test_values$install_3)\n\n#h2o classifier\ntrain_values_mod <- merge(train_values,train_labels,by = \"id\")\ntrain_values_mod <- train_values_mod[,-1]\ntrain_values_new <- train_values_mod\ntrain_values_mod <- subset(train_values_mod, select = c(amount_tsh,gps_height,longitude,latitude,basin,region_code,population,permit,construction_year,extraction_type,quantity_group,source_class,waterpoint_type,status_group))\nfolds <- cut(seq(1,nrow(train_values_mod)),breaks=5,labels=FALSE)\n#to start an active connection to an H2o cluster\n#h2o.init()\n#for(i in 1:5){\n  #Folds for 20 fold validation\n  #print(colnames(train_values_mod)[])\n#  index <- which(folds==i,arr.ind=TRUE)\n#  testData <- train_values_mod[index, ]\n#  trainData <- train_values_mod[-index, ]\n#  train_h2o <- as.h2o(trainData)\n#  test_h2o <- as.h2o(testData[,-14])\n#  model <- h2o.deeplearning(x = 1:13,y = 14,training_frame = train_h2o,activation = \"MaxoutWithDropout\",input_dropout_ratio = 0.1,hidden_dropout_ratios = c(0.1,0.1,0.1),balance_classes = TRUE,hidden = c(180,180,180),epochs = 100,fast_mode = TRUE)\n#  h2o_yhat_test <- h2o.predict(model, test_h2o)\n#  df_yhat_test <- as.data.frame(h2o_yhat_test)\n  #print(mean(pred==testLabels[, 2]))\n#  breakVector <- factor(df_yhat_test[,1],levels=levels(testData[,14]))\n#  print(mean(breakVector==testData[,14]))\n#}\n#random forest classifier\nset.seed(42)\nmodel_forest <- randomForest(as.factor(status_group) ~ longitude + latitude + extraction_type_group + quantity + waterpoint_type + construction_year + install_3,data = train, importance = TRUE,ntree =100, nodesize = 2)\nplot(model_forest)\n# Predict using the training values\npred_forest_train <- predict(model_forest, train)\n#importance(pred_forest_train) no applicable method for 'importance' applied to an object of class \"factor\"\nconfusionMatrix(pred_forest_train, train$status_group)\n# Predict using the test values\npred_forest_test <- predict(model_forest,test_values)\n# Create submission data frame\nsubmission <- data.frame(test_values$id)\nsubmission$status_group <- pred_forest_test\n#trying to compare the values of train and test data\ntable(submission$status_group)\ntable(train$status_group)\nprop.table(table(train$status_group))\nprop.table(table(submission$status_group))\nnames(submission)[1] <- \"id\"\n#give the file path\nwrite.csv(submission,file =\"Submission.csv\")\n\n#Bagging classifier\nseed <- 7\nmetric <- \"Accuracy\"\n# Bagged CART\nset.seed(seed)\nfit.treebag <- train(as.factor(status_group) ~ longitude + latitude + extraction_type_group + quantity + waterpoint_type + construction_year + install_3, data=train, method=\"treebag\", metric=metric)\nfit.treebag_prediction <- predict(fit.treebag,train)\nplot(fit.treebag)\nresults <- confusionMatrix(fit.treebag_prediction, train$status_group)\nfit.treebag_testpred <- predict(fit.treebag,test_values)\nresults\nsubmission_bag <-data.frame(test_values$id)\nsubmission_bag$status_group <- fit.treebag_testpred\n#trying to compare the values of train and test data\ntable(submission_bag$status_group)\nnames(submission_bag)[1] <- \"id\"\n#give the file path\nwrite.csv(submission_bag,file = \"Submission_bag.csv\")\n\n#Boosting classifier\nseed <- 7\nmetric <- \"Accuracy\"\nset.seed(seed)\nindex<-sample(1:nrow(train),round(0.05*nrow(train)))\ntrain_sample <- train[index,]\nfit.boost <- train(as.factor(status_group) ~ longitude + latitude + extraction_type_group + quantity + waterpoint_type + construction_year + install_3, data=train_sample, method=\"gbm\", metric=metric)\nfit.boost_prediction <- predict(fit.boost,train_sample)\n#plot(fit.treebag)\nresults <- confusionMatrix(fit.boost_prediction, train_sample$status_group)\nfit.boost_testpred <- predict(fit.boost,test_values)\nresults\nsubmission_boost <-data.frame(test_values$id)\nsubmission_boost$status_group <- fit.boost_testpred\n#trying to compare the values of train and test data\ntable(submission_boost$status_group)\nnames(submission_boost)[1] <- \"id\"\n#give the file path\nwrite.csv(submission_boost,file = \"Submission_boost.csv\")\n\n#knn classifier\ntrain_values_mod <- sapply(train_values,as.numeric)\ntrain_values_mod <- as.data.frame(train_values_mod)\ntrain_values_mod <- subset(train_values_mod, select = c(funder,installer,longitude,latitude,lga,ward,extraction_type,quantity_group,waterpoint_type))\nfolds <- cut(seq(1,nrow(train_values_mod)),breaks=20,labels=FALSE)\nfor(i in 1:20){\n  #Folds for 20 fold validation\n  index <- which(folds==i,arr.ind=TRUE)\n  testData <- train_values_mod[index, ]\n  testLabels <- sapply(train_lables[index, ],as.numeric)\n  trainData <- train_values_mod[-index, ]\n  trainLabels <- sapply(train_lables[-index, ],as.numeric)\n  pred<-knn(trainData, testData, trainLabels[,2], k = 5, prob=TRUE)\n  print(mean(pred==testLabels[, 2]))\n}",
    "created" : 1480347706551.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3496800644",
    "id" : "ACB403DF",
    "lastKnownWriteTime" : 1480347875,
    "last_content_update" : 1480347875095,
    "path" : "~/Downloads/FinalSourceCode.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}